<!DOCTYPE html><html lang="zh-cn"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>记一次JVM堆外内存泄漏问题 - Z.H.</title><meta name="description" content="问题现象 前段时间，公司线上的服务器开始出现pod反复重启的现象，但是没有JVM内存泄漏的日志，也没有找到OOM的dump文件，只有一条容器被kill的日志。通过普罗米修斯监控大盘，发现是JVM内存突破了pod的内存限制。 我们的应用服务是使用k8s来部署的，应用是基于jdk8的，出现问题的服务pod分配的总数为3台，每个pod分配3G内存和1个高性能CPU，pod的内存required和limit分别为2G和3G。 默认参数，未配置。 年轻代正常；老年代占用较多，可能有大对象存在；老年代内存增长同pod增长趋势相同，且有按小时增长迹象，并会触发FullGC。 保持上面的解决方案和参数，我们的服务勉强保持了一段时间，只是相对拉长了容器被重启的时间，告警和重启依然存在，实在令人头疼，很明显还是存在JVM内存泄漏。&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><script type="text/javascript" async src="https://www.googletagmanager.com/gtag/js?id=6694651183"></script><script type="text/javascript">window.dataLayer = window.dataLayer || [];
				  function gtag(){dataLayer.push(arguments);}
				  gtag('js', new Date());
				  gtag('config', '6694651183' );
				  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NRGQEGZN7K"></script><script>window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-NRGQEGZN7K');</script><link rel="canonical" href="https://jobslee0.github.io/ji-yi-ci-jvmdui-wai-nei-cun-xie-lou-wen-ti.html"><link rel="alternate" type="application/atom+xml" href="https://jobslee0.github.io/feed.xml"><link rel="alternate" type="application/json" href="https://jobslee0.github.io/feed.json"><meta property="og:title" content="记一次JVM堆外内存泄漏问题"><meta property="og:site_name" content="Z.H."><meta property="og:description" content="问题现象 前段时间，公司线上的服务器开始出现pod反复重启的现象，但是没有JVM内存泄漏的日志，也没有找到OOM的dump文件，只有一条容器被kill的日志。通过普罗米修斯监控大盘，发现是JVM内存突破了pod的内存限制。 我们的应用服务是使用k8s来部署的，应用是基于jdk8的，出现问题的服务pod分配的总数为3台，每个pod分配3G内存和1个高性能CPU，pod的内存required和limit分别为2G和3G。 默认参数，未配置。 年轻代正常；老年代占用较多，可能有大对象存在；老年代内存增长同pod增长趋势相同，且有按小时增长迹象，并会触发FullGC。 保持上面的解决方案和参数，我们的服务勉强保持了一段时间，只是相对拉长了容器被重启的时间，告警和重启依然存在，实在令人头疼，很明显还是存在JVM内存泄漏。&hellip;"><meta property="og:url" content="https://jobslee0.github.io/ji-yi-ci-jvmdui-wai-nei-cun-xie-lou-wen-ti.html"><meta property="og:type" content="article"><link rel="shortcut icon" href="https://jobslee0.github.io/media/website/photo2pixel_download-2.png" type="image/x-icon"><link rel="preload" href="https://jobslee0.github.io/assets/dynamic/fonts/jetbrainsmono/jetbrainsmono.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="https://jobslee0.github.io/assets/dynamic/fonts/jetbrainsmono/jetbrainsmono-italic.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="https://jobslee0.github.io/assets/css/style.css?v=82007ae11cb9f2a56571c770331b13e7"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://jobslee0.github.io/ji-yi-ci-jvmdui-wai-nei-cun-xie-lou-wen-ti.html"},"headline":"记一次JVM堆外内存泄漏问题","datePublished":"2023-01-10T16:22","dateModified":"2024-09-26T23:53","description":"问题现象 前段时间，公司线上的服务器开始出现pod反复重启的现象，但是没有JVM内存泄漏的日志，也没有找到OOM的dump文件，只有一条容器被kill的日志。通过普罗米修斯监控大盘，发现是JVM内存突破了pod的内存限制。 我们的应用服务是使用k8s来部署的，应用是基于jdk8的，出现问题的服务pod分配的总数为3台，每个pod分配3G内存和1个高性能CPU，pod的内存required和limit分别为2G和3G。 默认参数，未配置。 年轻代正常；老年代占用较多，可能有大对象存在；老年代内存增长同pod增长趋势相同，且有按小时增长迹象，并会触发FullGC。 保持上面的解决方案和参数，我们的服务勉强保持了一段时间，只是相对拉长了容器被重启的时间，告警和重启依然存在，实在令人头疼，很明显还是存在JVM内存泄漏。&hellip;","author":{"@type":"Person","name":"Jobs.Lee","url":"https://jobslee0.github.io/authors/jobslee/"},"publisher":{"@type":"Organization","name":"Jobs.Lee"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template"><div class="container container--center"><header class="header"><div class="header__logo"><a class="logo" href="https://jobslee0.github.io/">Z.H.</a></div></header><main class="content"><article class="post"><header><h1 class="post__title">记一次JVM堆外内存泄漏问题</h1><div class="post__meta"><time datetime="2023-01-10T16:22" class="post__date">一月 10, 2023 </time><span class="post__author"><a href="https://jobslee0.github.io/authors/jobslee/" class="feed__author">Jobs.Lee</a></span></div></header><div class="post__entry"><h3 id="问题现象">问题现象</h3><p>前段时间，公司线上的服务器开始出现pod反复重启的现象，但是没有JVM内存泄漏的日志，也没有找到OOM的dump文件，只有一条容器被kill的日志。通过普罗米修斯监控大盘，发现是JVM内存突破了pod的内存限制。</p><h3 id="问题分析">问题分析</h3><h4 id="现象分析">现象分析</h4><ol><li>硬件配置</li></ol><p>我们的应用服务是使用k8s来部署的，应用是基于jdk8的，出现问题的服务pod分配的总数为3台，每个pod分配3G内存和1个高性能CPU，pod的内存required和limit分别为2G和3G。</p><ol start="2"><li>JVM参数</li></ol><p>默认参数，未配置。</p><ol start="3"><li>JVM内存分析</li></ol><p>年轻代正常；老年代占用较多，可能有大对象存在；老年代内存增长同pod增长趋势相同，且有按小时增长迹象，并会触发FullGC。</p><h4 id="问题猜测">问题猜测</h4><ol><li>随着业务增长，pod内存可能确实不足</li><li>JVM默认参数无法有效限制JVM的内存使用</li><li>某个小时任务存在问题，且使用了大对象</li><li>大对象触发FullGC后并没有有效释放内存</li><li>查询k8s官方Issues，是否存在pod与Java版本兼容bug</li></ol><h4 id="尝试解决">尝试解决</h4><ol><li>增加pod内存，使得<code>required=4G</code> <code>limit=5G</code></li><li>增加JVM参数限制<code>-Xms=4G</code> <code>-Xmx=4G</code>，堆外内存也要限制，（特别注意jdk8为元空间）<code>-XX:MetaspaceSize=300m</code> <code>-XX:MaxMetaspaceSize=300m</code></li><li>翻查代码，确实发现某个定时任务存在着大批量的List内存存储，且长时间不释放，但是短时间内没有较好的修改替代方案</li><li>调整年轻代大小<code>-Xmn</code>，使大对象在年轻代就被回收，而不进入老年代</li><li>升级jdk8的小版本，<code>提升jvm对容器限制的感知</code>（这块见参考文档3）</li></ol><h4 id="尝试结果">尝试结果</h4><ol><li>JVM内存增长导致的重启次数变得减少一点，部分FullGC之后pod容器依然健在，但是时间久了依然会被kill重启</li><li>大对象没有被提前回收，依然进入了老年代，且FullGC明显增多</li></ol><h4 id="初步结论">初步结论</h4><ol><li>业务增长<strong>pod内存</strong>确实需要适当增加</li><li><strong>JVM参数</strong>需要进行手动限制，但是年轻代大小可以不需要调整，反而可以降低FullGC次数</li><li><strong>代码端</strong>想更好的优化方法，尽量让数据对象生命周期缩短，但是改动较难</li><li>jdk8小版本有对<strong>容器限制感知</strong>的修复，有一定用处（这块见参考文档3）</li></ol><h3 id="问题解决">问题解决</h3><p>保持上面的解决方案和参数，我们的服务勉强保持了一段时间，只是相对拉长了容器被重启的时间，告警和重启依然存在，实在令人头疼，很明显还是存在JVM内存泄漏。</p><p>后来通过不断的排查监控大盘，我发现堆内内存经过FullGC后，可以做到到达临界线后不再增长，这说明了<strong>堆内参数大小和限制</strong>都起到了很好的作用；但是要知道，JVM还有一部分叫做<strong>堆外内存</strong>的东西，参数中限制了我们常知道的<strong>MetaSpace</strong>，在大盘中我却发现了<strong>non-heap memory</strong>部分总有比MetaSpace多出的300M，其增长趋势同pod内存一样一致，只是不是那么明显。</p><p>这引发了我进一步的思考，堆外内存是不是除了<strong>元数据区</strong>，还有一块未曾想到的区域？</p><p>经过翻查资料：</p><figure class="post__image"><img loading="lazy" src="https://jobslee0.github.io/media/posts/4/181492b213cbc6520373818bd65550e8b23c7a.webp" alt="Image description" width="1706" height="818" sizes="(min-width: 920px) 703px, (min-width: 700px) calc(82vw - 35px), calc(100vw - 81px)" srcset="https://jobslee0.github.io/media/posts/4/responsive/181492b213cbc6520373818bd65550e8b23c7a-xs.webp 300w, https://jobslee0.github.io/media/posts/4/responsive/181492b213cbc6520373818bd65550e8b23c7a-sm.webp 480w, https://jobslee0.github.io/media/posts/4/responsive/181492b213cbc6520373818bd65550e8b23c7a-md.webp 768w, https://jobslee0.github.io/media/posts/4/responsive/181492b213cbc6520373818bd65550e8b23c7a-lg.webp 1024w"></figure><p>发现有一块叫做<strong>Direct Memory</strong>的区域，这块区域就是我们常提到的<strong>NIO</strong>为了减少内存拷贝而直接申请的部分，这部分在常见数据库读写客户端中大量存在。</p><p>通过排查代码，果不其然，代码除了保留List大对象之外，还大量使用了<strong>RedisTemplate</strong>对象，这个客户端底层连接<strong>基于netty实现</strong>，正是NIO那部分。</p><p>于是，加上<code>-MaxDirectMemorySize</code>参数后，再次测试，JVM内存稳固被限制，果然没有问题了！而这个参数正是《深入理解JVM虚拟机》前几页所提到的——很多程序员会忽略的一个参数。</p><h3 id="问题总结">问题总结</h3><p>总的来看，这次的问题DirectMemory是一个主要问题，代码的大对象处理是一个次要问题，其次才应该考虑对JVM的FullGC次数进行优化。</p><p>毕竟，JVM是一个复杂的大工程，它已经帮我们很好的完成了对内存的管理；出现了内存泄漏不可怕，要从现象和问题本身出发，要从JVM本身出发，万万不能被所谓的“八股文”和“权威”给束缚住，上来就考虑GC优化往往只会迷失方向。</p><p>那些被刻板经验给忽略的，往往就是真相，仅此而已。</p><hr><h5 id="参考文档">参考文档</h5><ol><li><a href="https://blog.csdn.net/tterminator/article/details/54342666">https://blog.csdn.net/tterminator/article/details/54342666</a></li><li><a href="https://zhuanlan.zhihu.com/p/370241822">https://zhuanlan.zhihu.com/p/370241822</a></li><li><a href="https://blog.51cto.com/lookingdream/4046529">https://blog.51cto.com/lookingdream/4046529</a></li><li><a href="https://juejin.cn/post/6844903894863052814%E2%80%8B%E2%80%8B">https://juejin.cn/post/6844903894863052814</a></li><li><a href="https://www.processon.com/view/link/5b61ea2ae4b0555b39cfa842">https://www.processon.com/view/link/5b61ea2ae4b0555b39cfa842</a></li></ol></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on 九月 26, 2024</p><div class="post__share"></div></footer><nav class="pagination"><div class="pagination__title"><span>Read other posts</span></div><div class="pagination__buttons"><a href="https://jobslee0.github.io/mongodbzhi-shi-zhan-wen-da.html" class="btn previous" rel="prev" aria-label="[MISSING TRANSLATION]:  MongoDB之实战问答 "><span class="btn__icon">←</span> <span class="btn__text">MongoDB之实战问答</span> </a><a href="https://jobslee0.github.io/psqdui-lie-jian-xi.html" class="btn next" rel="next" aria-label="[MISSING TRANSLATION]:  PSQ队列简析 "><span class="btn__text">PSQ队列简析</span> <span class="btn__icon">→</span></a></div></nav></article></main><footer class="footer"><div class="footer__inner"><div class="footer__copyright"><p>© 2024 Powered by Publii CMS :: <a href="https://github.com/panr/hugo-theme-terminal" target="_blank" rel="noopener">Theme</a> ported by the <a href="https://getpublii.com/customization-service/" target="_blank" rel="noopener">Publii Team</a></p></div></div></footer></div><script defer="defer" src="https://jobslee0.github.io/assets/js/scripts.min.js?v=c2232aa7558e9517946129d2a1b8c770"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:true,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>